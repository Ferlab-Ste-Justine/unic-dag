# üå¨Ô∏è unic-dag

This repository contains the DAG definitions for the **UnIC** datalake project (_Univers Informationnel du CHU Sainte-Justine_). These DAGs orchestrate ETL jobs defined in the [unic-etl repository](https://github.com/Ferlab-Ste-Justine/unic-etl).

---

## ‚öôÔ∏è Requirements

- üêç Python 3.9

### Setup with a virtual environment
```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```
---
## üõ†Ô∏è DAG configuration
DAGs can be defined in **two ways**:
1. JSON configuration files located in `dags/config`.
2. Python DAGs directly in the `dags` folder for advanced use cases.

### üìÑ JSON configuration files
Each JSON file in `dags/config` defines one DAG to orchestrate a single **resource**. The file must follow this naming and placement convention:
- The **filename** must start with the **resource code** and end with `_config.json`
- The file must be placed under the folder for its **starting zone**:  
  - `red` or `yellow`
- And inside the corresponding **starting subzone** folder:  
  - `red` ‚Üí `ingestion`, `curated`, `enriched`  
  - `yellow` ‚Üí `anonymized`, `enriched`, `warehouse`

The generated DAG ID will follow this pattern:

```
<subzone>_<resource code>
```

> Example:  
> A file named `dags/config/red/curated/opera_config.json` will create a DAG named `curated_opera`.

#### JSON Configuration Fields
| Field           | Type   | Required | Default        | Description                                    |
| --------------- | ------ | -------- |----------------|------------------------------------------------|
| `concurrency`   | int    | ‚ùå        | `1`            | Max number of parallel tasks.                  |
| `start_date`    | string | ‚ùå        | `"2021-01-01"` | Initial DAG run date (format: `"YYYY-MM-DD"`). |
| `schedule`      | string | ‚ùå        | ‚Äì              | CRON expression for scheduling.                |
| `catchup`       | bool   | ‚ùå        | `false`        | Whether to backfill missed DAG runs.           |
| `timeout_hours` | int    | ‚ùå        | `4`            | Max execution time in hours before timeout.    |
| `steps`         | array  | ‚úÖ        | ‚Äì              | List of execution steps. See below.            |


<details> <summary>Each <b>step</b> can contain</summary>

| Step Field              | Type   | Required | Default | Description                                                                               |
| ----------------------- | ------ | -------- | ------- |-------------------------------------------------------------------------------------------|
| `destination_zone`      | string | ‚úÖ        | ‚Äì       | Target zone (e.g., `"red"`, `"yellow"`, `"green"`).                                       |
| `destination_subzone`   | string | ‚úÖ        | ‚Äì       | Subzone within the zone (e.g., `"curated"`, `"released"`).                                |
| `main_class`            | string | ‚ùå        | ‚Äì       | Main class to run in `unic-etl`. Optional for published tasks.                            |
| `multiple_main_methods` | bool   | ‚ùå        | `false` | Whether multiple entrypoints exist in the main class.                                     |
| `pre_tests`             | array  | ‚ùå        | `[]`    | QA tests before step execution. See below.                                                |
| `datasets`              | array  | ‚úÖ        | ‚Äì       | List of datasets to process in the step. One task per dataset will be created. See below. |
| `post_tests`            | array  | ‚ùå        | `[]`    | QA tests after step execution. See below.                                                 |
| `optimize`              | array  | ‚ùå        | `[]`    | List of dataset IDs to optimize (i.e. Delta compaction).                                  |
</details>

<details> <summary>Each <b>dataset</b> can contain</summary>

| Dataset Field  | Type   | Required | Default | Description                                                                                                                                    |
| -------------- | ------ | -------- |---------|------------------------------------------------------------------------------------------------------------------------------------------------|
| `dataset_id`   | string | ‚úÖ        | ‚Äì       | Dataset ID. Supports wildcards.                                                                                                                |
| `cluster_type` | string | ‚ùå        | ‚Äì       | Cluster size (`"xsmall"`, `"small"`, `"medium"` or `"large"`). Optional for published tasks.                                                   |
| `run_type`     | string | ‚ùå        | ‚Äì       | Execution type (`"default"` or `"initial"` to reset data). Optional for published tasks.                                                       |
| `pass_date`    | bool   | ‚ùå        | ‚Äì       | Whether to pass the execution date as a `--date` parameter (for enriched tasks) or a `--version` parameter (for released and published tasks). |
| `dependencies` | array  | ‚úÖ        | -       | List of dataset IDs to run upstream. Set to `[]` if there are no upstream dependencies.                                                        |
</details>


<details> <summary>Each <b>test</b> (pre or post) must contain</summary>

| Test Field     | Type   | Required | Default | Description                                                                                                                                  |
|----------------| ------ | -------- |---------|----------------------------------------------------------------------------------------------------------------------------------------------|
| `name`         | string | ‚úÖ        | ‚Äì       | Name of the QA test in `unic-etl`.                                                                                                           |
| `destinations` | string | ‚úÖ        | ‚Äì       | List of dataset IDs to test. Supports wildcards.                                                                                             |
</details>

---

## ‚ñ∂Ô∏è Starting a DAG from the Airflow UI

When manually triggering a DAG in the Airflow UI, you'll see two default input parameters:

| Parameter | Default     | Description                                                                                                    |
|-----------|-------------|----------------------------------------------------------------------------------------------------------------|
| `branch`  | `master`    | Selects the JAR file to run, corresponding to the branch used to deploy the JAR (e.g., `unic-etl-master.jar`). |
| `version` | `latest`    | For **enriched** DAGs: set to a date (`YYYY-MM-dd`) to publish a specific version.                                 |

---
## üê≥ Run Airflow Locally with Docker

#### 1. Create `.env` file

```bash
cp .env.sample .env
```

#### 2. Deploy the stack

```bash
docker-compose up
```

#### 3. Access the Airflow UI

- URL : `http://localhost:50080`
- Username : `airflow`
- Password : `airflow`

#### 4. Set Airflow Variables
Navigate to **Admin ‚Üí Variables** and add:
- dags_path : `/opt/airflow/dags`
- base_url (optional) : `http://localhost:50080`

To speed up setup, upload the `variables.json` file directly from the UI.

#### 5. Test a task
Run a task manually from the Airflow UI or use the CLI:
```bash
docker-compose exec airflow-scheduler airflow tasks test <dag> <task> 2022-01-01
```
---


### üì¶ Minio Setup _(Optional)_

#### 1. Access the Minio UI

- URL : `http://localhost:59001`
- Username : `minioadmin`
- Password : `minioadmin`

#### 2. Set Airflow Variable
Navigate to **Admin ‚Üí Variables** and add:

- s3_conn_id : `minio`

#### 3. Set Airflow Connection
Navigate to **Admin ‚Üí Connections** and add:

- Connection ID : `minio`
- Connection Type : `Amazon S3`
- Extra :

```json
{
  "host": "http://minio:9000",
  "aws_access_key_id": "minioadmin",
  "aws_secret_access_key": "minioadmin"
}
```
---


### üì¶ Postgres Setup _(Optional)_

#### 1. Access the PgAdmin UI

- URL : `http://localhost:5050`
- Username : `pgadmin@pgadmin.com`
- Password : `pgadmin`

#### 2. Set Airflow Variable
Navigate to **Admin ‚Üí Variables** and add:

- pg_conn_id : `postgres` (Set to corresponding Postgres connection ID. Eg: `unic_dev_postgresql_vlan2_rw`)

#### 3. Set Airflow Connection
Navigate to **Admin ‚Üí Connections** and add:

- Connection ID : `postgres` (Set to corresponding Postgres connection ID. Eg: `unic_dev_postgresql_vlan2_rw`)
- Connection Type : `Postgres`
- Host: `postgres-unic`
- Schema: `unic`
- Password: `pgadmin`
- Port: `5432`

---
### üîî Slack Setup _(Optional)_
#### 1. Set Slack Hook Variable
Navigate to **Admin ‚Üí Variables** and add:

- slack_hook_url : `https://hooks.slack.com/services/...`